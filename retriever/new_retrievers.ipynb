{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "# 1. Load entire dataset\n",
    "corpus_name = 'kilt_wikipedia'\n",
    "corpus_file_name = f'/data/tir/projects/tir6/general/afreens/dbqa/data/corpus_files/{corpus_name}/{corpus_name}_jsonl/{corpus_name}.jsonl'\n",
    "# corpus_dataset = load_dataset('json', data_files=corpus_file_name)\n",
    "\n",
    "# 2. For dev load subseet (50) of dataset\n",
    "passages = []\n",
    "with open(corpus_file_name, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        data = json.loads(line)\n",
    "        passages.append({'id': data['id'], 'contents': data['contents']})\n",
    "        if i == 50:\n",
    "            break\n",
    "from datasets import Dataset\n",
    "corpus_dataset = Dataset.from_list(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'contents'],\n",
       "    num_rows: 51\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filename, sort_by_id = True):\n",
    "    print('loading from', filename)\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())\n",
    "            data.append(json_obj)\n",
    "    if sort_by_id:\n",
    "        for d in data:\n",
    "            d[\"id\"] = str(d[\"id\"])\n",
    "        return sorted(data, key=lambda x: x['id'])\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data, filename):\n",
    "    if os.path.dirname(filename):\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok= True)\n",
    "    print('writing to', filename)\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for idx, element in enumerate(data):\n",
    "            json.dump(element, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "            \n",
    "def get_retriever_outputs(corpus_dataset, query_dataset, top_k):\n",
    "    retrieved_output = []\n",
    "    for i, q in enumerate(query_dataset):\n",
    "        query = q['input']\n",
    "        indices, distances = retrieve(query, top_k)\n",
    "        provenance = []\n",
    "        for (i,d) in zip(indices, distances):\n",
    "            id = corpus_dataset[(int)(i)]['id']\n",
    "            page_id = id.split('_')[0]\n",
    "            start_par_id = id.split('_')[1]\n",
    "            end_par_id = start_par_id\n",
    "            content = corpus_dataset[(int)(i)]['contents']\n",
    "            score = (float)(1/d)\n",
    "\n",
    "            provenance.append({\"page_id\": page_id,\n",
    "            \"start_par_id\": start_par_id,\n",
    "            \"end_par_id\": end_par_id,\n",
    "            \"text\": content,\n",
    "            \"score\":score })\n",
    "        \n",
    "        retrieved_output.append({\"id\": q['id'], \n",
    "        \"input\": q['input'], \n",
    "        \"output\": [{\"provenance\": provenance}]})\n",
    "        return retrieved_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the question encoder and tokenizer\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# Load the context encoder and tokenizer\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d6a8e04d2a41fca8530042f24a0720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def encode_passages(examples):\n",
    "    inputs = context_tokenizer(examples['contents'], return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = context_encoder(**inputs).pooler_output.cpu().numpy()\n",
    "    return {'embeddings': embeddings}\n",
    "\n",
    "# Encode passages in bulk\n",
    "encoded_corpus_dataset = corpus_dataset.map(encode_passages, batched=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4eedc5e855406ab5cc8a7d0ef898a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever_name = 'dpr'\n",
    "encoded_corpus_dataset.save_to_disk(os.path.join(os.getenv(\"DBQA\"), 'indexes',  retriever_name, corpus_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = np.vstack(encoded_corpus_dataset['embeddings']).astype(np.float32)\n",
    "index = faiss.IndexFlatIP(context_embeddings.shape[1]) \n",
    "index.add(context_embeddings)\n",
    "passage_ids = encoded_corpus_dataset['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 25  3 45  8] [54.289043 54.14091  52.687218 51.875103 51.52078 ]\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, top_k=5):\n",
    "    # Encode the query using the question encoder\n",
    "    inputs = question_tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = question_encoder(**inputs)\n",
    "        # query_embedding = mean_pooling(outputs[0], inputs['attention_mask']).cpu().numpy()\n",
    "        query_embedding = question_encoder(**inputs).pooler_output.cpu().numpy()\n",
    "\n",
    "    # Perform the retrieval\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    return I[0], D[0]\n",
    "\n",
    "# Example query\n",
    "query = \"What is the capital of France?\"\n",
    "top_k = 5\n",
    "indices, distances = retrieve(query, top_k)\n",
    "print(indices, distances)\n",
    "# Print the top-k retrieved passages\n",
    "# for idx, distance in zip(indices, distances):\n",
    "#     print(f\"ID: {passage_ids[idx]}\\nPassage: {dataset[idx]['contents']}\\nScore: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /data/tir/projects/tir6/general/afreens/dbqa/data/nq-dev-kilt.jsonl\n"
     ]
    }
   ],
   "source": [
    "# load query dataset\n",
    "query_dataset_name = 'nq-dev-kilt'\n",
    "query_file_name = f'/data/tir/projects/tir6/general/afreens/dbqa/data/{query_dataset_name}.jsonl'\n",
    "query_dataset = load_jsonl(query_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_output = get_retriever_outputs(corpus_dataset, query_dataset, top_k)\n",
    "save_jsonl(retrieved_output, os.path.join(os.getenv(\"DBQA\"), 'retriever_results', 'predictions', retriever_name, query_dataset_name + '.jsonl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b453516b744409b79a6072def4a2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4e6a1cc8084e839eda7c71be0ed38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03c35c34f164f478f50afd9a17a8868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542624c4430e48289f0c3ee7aa3b6504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3ebeab057c4cfa88561a0291df8dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ddfb3da09b43789596ded826cd2c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaec1d1bb944b52bbfd253b96ad1fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8159c7f68a524b0e8d10d61ba49964ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the GTR model and tokenizer\n",
    "model_name = \"sentence-transformers/gtr-t5-base\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51964b823079470dbf42c0255e0a4993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "passages = dataset['contents']\n",
    "context_embeddings = model.encode(passages, convert_to_numpy=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = context_embeddings.astype(np.float32)\n",
    "index = faiss.IndexFlatIP(context_embeddings.shape[1]) \n",
    "index.add(context_embeddings)\n",
    "passage_ids = dataset['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32  6  4 10  0] [0.56198996 0.5037815  0.48101538 0.480132   0.4723112 ]\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, top_k=5):\n",
    "    # Encode the query using the GTR model\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype(np.float32)\n",
    "\n",
    "    # Perform the retrieval\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    return I[0], D[0]\n",
    "\n",
    "# Example query\n",
    "query = \"What is the capital of France?\"\n",
    "top_k = 5\n",
    "indices, distances = retrieve(query, top_k)\n",
    "print(indices, distances)\n",
    "# # Print the top-k retrieved passages\n",
    "# for idx, distance in zip(indices, distances):\n",
    "#     print(f\"ID: {passage_ids[idx]}\\nPassage: {dataset[idx]['contents'][idx]}\\nScore: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the Contriever model and tokenizer\n",
    "model_name = \"facebook/contriever\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3564b6264724a348bb9e44fcff1f86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "    \n",
    "def encode_passages(examples):\n",
    "    inputs = tokenizer(examples['contents'], return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        # embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        # embeddings = model(**inputs).pooler_output.cpu().numpy()\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "    return {'embeddings': embeddings}\n",
    "\n",
    "# Encode passages in bulk\n",
    "encoded_dataset = dataset.map(encode_passages, batched=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = np.vstack(encoded_dataset['embeddings']).astype(np.float32)\n",
    "index = faiss.IndexFlatIP(context_embeddings.shape[1]) \n",
    "index.add(context_embeddings)\n",
    "passage_ids = encoded_dataset['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32 21 42 20 48] [0.703585   0.6459863  0.63183594 0.59237474 0.58883893]\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, top_k=5):\n",
    "    # Encode the query using the Contriever model\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = mean_pooling(outputs[0], inputs['attention_mask']).cpu().numpy()\n",
    "        # query_embedding = model(**inputs).pooler_output.cpu().numpy()\n",
    "        # query_embedding = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    # Perform the retrieval\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    return I[0], D[0]\n",
    "\n",
    "# Example query\n",
    "query = \"What is the capital of France?\"\n",
    "top_k = 5\n",
    "indices, distances = retrieve(query, top_k)\n",
    "print(indices, distances)\n",
    "# Print the top-k retrieved passages\n",
    "# for idx, distance in zip(indices, distances):\n",
    "#     print(f\"Passage: {passages[idx]}\\nScore: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
